# **MACHINE LEARNING**
# CHargement des packages
import numpy as np
import pandas as pd
df = pd.read_csv("C:/Users/aurel/Documents/Python Scripts/Machine learning/housing.csv")
df
## Regression
df.head()
#### Exploration des données
# Dimensions du df
print("Dimension du df :", df.shape, "\n")
print(df.info())
# Stats descriptives
df.describe()
#### Analyse de données
# On va se focaliser sur notre variable cible
import seaborn as sns # extension de Matplotlib. Seaborn facilite la création de graphiques statistiques attrayants et informatifs.
import matplotlib.pyplot as plt # Matplotlib est une bibliothèque de visualisation de données en Python. L'utilisation de l'alias plt est une convention courante.

# Importation de certaines fonctionnalités de Matplotlib. Ces lignes importent des modules spécifiques de Matplotlib qui peuvent être utilisés pour manipuler
# et personnaliser certains aspects des graphiques, tels que les lignes et les transformations.
import matplotlib.lines as mlines
import matplotlib.transforms as mtransforms

%matplotlib inline 
# Afficher les graphiques générés par Matplotlib directement dans le notebook
# plutôt que dans une fenêtre séparée. Cela est particulièrement utile pour l'exploration de données interactive dans un notebook.
# Distribution des prix
sns.displot(df["Price"], kde = True)
sns.set_style("whitegrid")  # Optionnel : définir un style pour le graphique
plt.pyplot.title("Distribution de la variable cible Price")
plt.pyplot.show()
fig, ax = plt.subplots() # Créer une figure et des axes
ax.scatter(df["Avg. Area Income"], df["Price"], s = 5) # Nuage de points (scatter plot) avec une taille de points ajustée (s)
line = mlines.Line2D([0, 1], [0, 1], color='red') # Créer une ligne diagonale
transform = ax.transAxes # Définir la transformation pour la ligne
line.set_transform(transform) 
ax.add_line(line) # Ajouter la ligne au graphique
plt.xlabel("Revenu moyen dans la région") # Ajouter des labels et un titre
plt.ylabel("Prix")
plt.title("Evolution du prix en fonction du revenu moyen dans la région")
plt.show() # Afficher le graphique
# Exclure la colonne "Address" du DataFrame avant de calculer la corrélation
df_corr = df.drop("Address", axis=1)
# Créer une heatmap de la matrice de corrélation avec Seaborn
sns.heatmap(df_corr.corr())
# Ajouter un titre avec Matplotlib
plt.title("Matrice de corrélation des variables du dataframe")
# Afficher le graphique
plt.show()
# Coeff de pearson installation packages
from scipy.stats import pearsonr
# Calcul du coefficient de corrélation de Pearson et p-value
result = pd.DataFrame(pearsonr(df["Price"], df["Avg. Area Income"]), index=['pearson_coeff', 'p-value'], columns=['Test de Pearson'])
# Afficher le résultat
result
#### Cleaning des données et preprocessing
# Un algo de machine learning va pas tourner si on a des N/A
print("Nombre de valeurs N/A : \n", df.isna().sum())
print("\n")
print("Nombre de doublons : \n", df.duplicated().sum())
**S'il y a avait des variables dichotomiques (Yes/No), pour les coder en binaire :**

df["var_name"] = df["var_name"], replace(["no", "yes"], [0, 1])
y = df["Price"]  # Variable à prédire (apprentissage supervisé)
x = df.drop(["Price", "Address"], axis=1)  # Toutes les variables sauf le prix et l'adresse pour prédire le prix
x
On va découper notre jeu de données en ensemble d'entrainement et en ensemble de test
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123)
# 20% de nos données sont dans l'ensemble de test et 80% en entrainement (test_size = 0.2)
from sklearn.preprocessing import StandardScaler

# Standardisation des variables (les remettre à la même échelle)
scaler = StandardScaler()
num = ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population']
x_train[num] = scaler.fit_transform(x_train[num])
x_test[num] = scaler.transform(x_test[num])
***fit_transform sur l'ensemble d'entraînement*** : Lorsque vous appelez fit_transform sur l'ensemble d'entraînement, le StandardScaler calcule la moyenne et l'écart-type des caractéristiques (features) dans cet ensemble, puis applique la transformation (standardisation) aux données.

***transform sur l'ensemble de test*** : Pour l'ensemble de test, vous ne souhaitez pas recalculer la moyenne et l'écart-type, car cela pourrait introduire une fuite d'informations. Vous utilisez plutôt les valeurs calculées sur l'ensemble d'entraînement. C'est pourquoi vous utilisez transform sur l'ensemble de test, en appliquant la même transformation (moyenne et écart-type) que celle calculée sur l'ensemble d'entraînement.
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
# Créer une instance du modèle de régression linéaire
reg = LinearRegression()
# Entraîner le modèle sur l'ensemble d'entraînement
reg.fit(x_train, y_train)
# Faire des prédictions sur l'ensemble de test
y_pred = reg.predict(x_test)
# Évaluer les performances du modèle
print(f"Score : {round(r2_score(y_test, y_pred), 2)}") # Tjrs entre 0 et 1
print('\n')
# Afficher les prédictions
print("Prédictions de y_test :", y_pred)
fig, ax = plt.subplots()
ax.scatter(y_test, y_pred, s = 5)
line = mlines.Line2D([0, 1], [0, 1], color = 'red')
transform = ax.transform = ax.transAxes
line.set_transform(transform)
ax.add_line(line)
plt.title("Vrais prix vs prix prédits")
plt.show()
